{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Suraj\\\\Desktop\\\\Python\\\\source-code-analysis-gen-ai\\\\research'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_dir/\"\n",
    "repo = Repo.clone_from(\"https://github.com/suryanshp1/Gem-Price-Prediction-end-to-end-pipeline\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(repo_path,\n",
    "                                        glob = \"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_dir\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, render_template, request\\nfrom src.pipeline.prediction_pipeline import PredictPipeline, CustomData\\nfrom src.exception.exception import CustomException\\nfrom src.logger.logger import logging\\nimport sys\\n\\napp=Flask(__name__)\\n\\n@app.route(\"/\")\\ndef home_page():\\n    try:\\n        return render_template(\\'index.html\\')\\n    except Exception as e:\\n        raise CustomException(e,sys)\\n    \\n@app.route(\"/predict\", methods=[\\'POST\\', \"GET\"])\\ndef predict_datapoint():\\n    try:\\n        if request.method==\"GET\":\\n            return render_template(\\'form.html\\')\\n        else:\\n            data=CustomData(\\n                carat=float(request.form.get(\"carat\")),\\n                depth=float(request.form.get(\"depth\")),\\n                table=float(request.form.get(\"table\")),\\n                x=float(request.form.get(\"x\")),\\n                y=float(request.form.get(\"y\")),\\n                z=float(request.form.get(\"z\")),\\n                cut=request.form.get(\"cut\"),\\n                color=request.form.get(\"color\"),\\n                clarity=request.form.get(\"clarity\")\\n            )\\n\\n            final_data = data.get_data_as_dataframe()\\n            pred_pipeline=PredictPipeline()\\n            pred=pred_pipeline.predict(final_data)\\n\\n            result = round(pred[0],2)\\n            return render_template(\\'result.html\\', final_result=result)\\n\\n    except Exception as e:\\n        raise CustomException(e,sys)\\n    \\nif __name__==\"__main__\":\\n    app.run(host=\"0.0.0.0\", port=80, debug=True)'),\n",
       " Document(metadata={'source': 'test_dir\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content='from setuptools import find_packages,setup\\nfrom typing import List\\n\\n\\nHYPHEN_E_DOT = \"-e .\"\\n\\n\\ndef get_requirements(filepath: str) -> List[str]:\\n    requirements = []\\n    with open(filepath) as file_obj:\\n        requirements = file_obj.readlines()\\n        requirements = [req.replace(\"\\\\n\", \"\") for req in requirements]\\n\\n        if HYPHEN_E_DOT in requirements:\\n            requirements.remove(HYPHEN_E_DOT)\\n\\n    return requirements\\n\\nsetup(\\n    name=\\'GemPricePrediction\\',\\n    version=\\'0.0.1\\',\\n    author=\\'Suryansh Pandey\\',\\n    author_email=\\'suryanshp1@gmail.com\\',\\n    install_requires=get_requirements(\"requirements.txt\"),\\n    packages=find_packages()\\n)'),\n",
       " Document(metadata={'source': 'test_dir\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom pathlib import Path\\n\\n\\nlist_of_files = [\\n    \".github/workflows/.gitkeep\",\\n    \"src/__init__.py\",\\n    \"src/components/__init__.py\",\\n    \"src/components/data_ingestion.py\",\\n    \"src/components/data_transformation.py\",\\n    \"src/components/model_trainer.py\",\\n    \"src/components/model_evaluation.py\",\\n    \"src/pipeline/__init__.py\",\\n    \"src/pipeline/training_pipeline.py\",\\n    \"src/pipeline/prediction_pipeline.py\",\\n    \"src/utils/__init__.py\",\\n    \"src/utils/utils.py\",\\n    \"src/logger/__init__.py\",\\n    \"src/logger/logger.py\",\\n    \"src/exception/__init__.py\",\\n    \"src/exception/exception.py\",\\n    \"tests/__init__.py\",\\n    \"tests/unit/__init__.py\",\\n    \"tests/integration/__init__.py\",\\n    \"init_setup.sh\",\\n    \"requirements.txt\",\\n    \"requirements_dev.txt\",\\n    \"setup.py\",\\n    \"pyproject.toml\",\\n    \"setup.cfg\",\\n    \"tox.ini\",\\n    \"experiment/experiments.ipynb\"\\n]\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)\\n\\n    if filedir != \"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        # logging.info(f\"Creating directory: {filedir} for file: {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass'),\n",
       " Document(metadata={'source': 'test_dir\\\\airflow\\\\dags\\\\batch_prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content='\"\"\"from asyncio import tasks\\nimport json\\nfrom textwrap import dedent\\nimport pendulum\\nimport os\\nfrom airflow import DAG\\nfrom airflow.operators.python import PythonOperator\\n\\nwith DAG(\\n    \\'batch_prediction\\',\\n    default_args={\\'retries\\': 2},\\n    # [END default_args]\\n    description=\\'gemstone batch prediction\\',\\n    schedule_interval=\"@weekly\", # here you can test based on hour or mints but make sure here you container is up and running\\n    start_date=pendulum.datetime(2023,4, 11, tz=\"UTC\"),\\n    catchup=False,\\n    tags=[\\'example\\'],\\n) as dag:\\n    def download_files(**kwargs):\\n        bucket_name = os.getenv(\"BUCKET_NAME\")# download the file from the repo\\n        input_dir = \"/app/input_files\"\\n        #creating directory\\n        os.makedirs(input_dir,exist_ok=True)\\n        #os.system(f\"aws s3 sync s3://{bucket_name}/inbox {config.inbox_dir}\")\\n\\n    def batch_prediction(**kwargs):\\n        config = BatchPredictionConfig()\\n        sensor_batch_prediction = SensorBatchPrediction(batch_config=config)\\n        sensor_batch_prediction.start_prediction()\\n       \\n    def upload_files(**kwargs):\\n        #bucket_name = os.getenv(\"BUCKET_NAME\")\\n        #os.system(f\"aws s3 sync {config.archive_dir} s3://{bucket_name}/archive\")\\n        #os.system(f\"aws s3 sync {config.outbox_dir} s3://{bucket_name}/outbox\")\\n\\n\\n   # download_input_files  = PythonOperator(\\n            task_id=\"download_file\",\\n            python_callable=download_files\\n\\n    )\\n\\n    generate_prediction_files = PythonOperator(\\n            task_id=\"prediction\",\\n            python_callable=batch_prediction\\n\\n    )\\n\\n    upload_prediction_files = PythonOperator(\\n            task_id=\"upload_prediction_files\",\\n            python_callable=upload_files\\n\\n    )\\n\\n    download_input_files >> generate_prediction_files >> upload_prediction_files\\n    \\n    \"\"\"'),\n",
       " Document(metadata={'source': 'test_dir\\\\airflow\\\\dags\\\\training_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='from __future__ import annotations\\nfrom textwrap import dedent\\nimport pendulum\\nimport subprocess\\nfrom airflow import DAG\\nfrom airflow.operators.python import PythonOperator\\nfrom src.pipeline.training_pipeline import TrainingPipeline\\n\\n\\ntraining_pipeline = TrainingPipeline()\\n\\nwith DAG(\\n    dag_id=\"gemstone_training_pipeline\",\\n    default_args={\\n        \"retries\": 3,\\n    },\\n    description=\"Training pipeline for gemstone dataset\",\\n    schedule=\"@weekly\",\\n    start_date=pendulum.datetime(2024, 10, 6, tz=\"UTC\"),\\n    catchup=False,\\n    tags=[\"training\", \"machine_learning\", \"classification\"],\\n) as dag:\\n    \\n    dag.doc_md = __doc__\\n\\n    def data_ingestion(**kwargs):\\n        ti = kwargs.get(\"ti\")\\n        train_data_path, test_data_path = training_pipeline.start_data_ingestion()\\n        ti.xcom_push(key=\"data_ingestion_artifact\", value={\"train_data_path\": train_data_path, \"test_data_path\": test_data_path})\\n\\n    def data_transformation(**kwargs):\\n        ti = kwargs.get(\"ti\")\\n        data_ingestion_artifact = ti.xcom_pull(key=\"data_ingestion_artifact\", task_ids=\"data_ingestion\")\\n        train_arr, test_arr = training_pipeline.start_data_transformation(data_ingestion_artifact.get(\"train_data_path\"), data_ingestion_artifact.get(\"test_data_path\"))\\n        train_arr=train_arr.tolist()\\n        test_arr=test_arr.tolist()\\n        ti.xcom_push(key=\"data_transformation_artifact\", value={\"train_arr\": train_arr, \"test_arr\": test_arr})\\n\\n    def model_trainer(**kwargs):\\n        import numpy as np\\n\\n        ti = kwargs.get(\"ti\")\\n        data_transformation_artifact = ti.xcom_pull(key=\"data_transformation_artifact\", task_ids=\"data_transformation\")\\n\\n        train_arr = np.array(data_transformation_artifact.get(\"train_arr\"))\\n        test_arr = np.array(data_transformation_artifact.get(\"test_arr\"))\\n\\n        training_pipeline.start_model_trainer(train_arr, test_arr)\\n\\n    def push_data_to_azureblob(**kwargs):\\n        # first configure azure cli in environment\\n        account_name = \"<account-name>\"\\n        destination_path = \"<destination-path>\"\\n        source_path = \"/app/artifacts\"\\n        # save it to the azure blob\\n        subprocess.run(f\"az storage blob upload-batch --account-name {account_name} --destination-path {destination_path} --source {source_path}\", shell=False)\\n\\n    data_ingestion_task = PythonOperator(\\n        task_id=\"data_ingestion\",\\n        python_callable=data_ingestion,\\n    )\\n    data_ingestion_task.doc_md = dedent(\\n        \"\"\"\\\\\\n    #### Ingestion task\\n    this task creates a train and test file.\\n    \"\"\"\\n    )\\n\\n    data_transformation_task = PythonOperator(\\n        task_id=\"data_transformation\",\\n        python_callable=data_transformation,\\n    )\\n    data_transformation_task.doc_md = dedent(\\n        \"\"\"\\\\\\n    #### Transformation task\\n    this task performs the transformation\\n    \"\"\"\\n    )\\n\\n    model_trainer_task = PythonOperator(\\n        task_id=\"model_trainer\",\\n        python_callable=model_trainer,\\n    )\\n    model_trainer_task.doc_md = dedent(\\n        \"\"\"\\\\\\n    #### model trainer task\\n    this task perform training\\n    \"\"\"\\n    )\\n\\n\\n    # push_data_to_azureblob_task = PythonOperator(\\n    #     task_id=\"push_data_to_azureblob\",\\n    #     python_callable=push_data_to_azureblob,\\n    # )\\n\\n    # push_data_to_azureblob_task.doc_md = dedent(\\n    #     \"\"\"\\\\\\n    # #### push data to azureblob task\\n    # this task push data to azureblob\\n    # \"\"\"\\n    # )\\n\\n    data_ingestion_task >> data_transformation_task >> model_trainer_task'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='import pandas as pd\\nimport numpy as np\\nfrom src.logger.logger import logging\\nfrom src.exception.exception import CustomException\\n\\nimport os\\nimport sys\\nfrom sklearn.model_selection import train_test_split\\nfrom dataclasses import dataclass\\nfrom pathlib import Path\\n\\n@dataclass\\nclass DataIngestionConfig:\\n    raw_data_path = os.path.join(\"artifacts\", \"raw.csv\")\\n    train_data_path = os.path.join(\"artifacts\", \"train.csv\")\\n    test_data_path = os.path.join(\"artifacts\", \"test.csv\")\\n\\nclass DataIngestion:\\n    def __init__(self):\\n        self.ingestion_config = DataIngestionConfig()\\n\\n    def initiate_data_ingestion(self):\\n        logging.info(\"Started the data ingestion.....\")\\n        try:\\n            logging.info(\"Entered the data ingestion method or component\")\\n            data = pd.read_csv(\"https://raw.githubusercontent.com/suryanshp1/Gem-Price-Prediction-end-to-end-pipeline/refs/heads/main/data/train.csv\")\\n            logging.info(\"Reading the dataset as dataframe\")\\n\\n            os.makedirs(os.path.dirname(self.ingestion_config.raw_data_path), exist_ok=True)\\n            data.to_csv(self.ingestion_config.raw_data_path, index=False)\\n            logging.info(\"Saved the raw data in artifact folder\")\\n            logging.info(\"Train test split initiated\")\\n\\n            train_data, test_data = train_test_split(data, test_size=0.25)\\n\\n            logging.info(\"Train test split completed\")\\n\\n            train_data.to_csv(self.ingestion_config.train_data_path, index=False)\\n            test_data.to_csv(self.ingestion_config.test_data_path, index=False)\\n\\n            logging.info(\"Data ingestion of data completed\")\\n\\n            return (\\n                self.ingestion_config.train_data_path,\\n                self.ingestion_config.test_data_path,\\n            )\\n\\n        except Exception as e:\\n            logging.info(e)\\n            raise CustomException(e, sys)\\n\\n# if __name__ == \"__main__\":\\n#     obj = DataIngestion()\\n#     train_data, test_data = obj.initiate_data_ingestion()'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='import pandas as pd\\nimport numpy as np\\nfrom src.logger.logger import logging\\nfrom src.exception.exception import CustomException\\nimport os\\nimport sys\\nfrom dataclasses import dataclass\\nfrom pathlib import Path\\n\\n\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import OrdinalEncoder,StandardScaler\\n\\nfrom src.utils.utils import save_object\\n\\n@dataclass\\nclass DataTransformationConfig:\\n    preprocessor_obj_file_path=os.path.join(\\'artifacts\\',\\'preprocessor.pkl\\')\\n\\n\\nclass DataTransformation:\\n    def __init__(self):\\n        self.data_transformation_config=DataTransformationConfig()\\n\\n        \\n    \\n    def get_data_transformation(self):\\n        \\n        try:\\n            logging.info(\\'Data Transformation initiated\\')\\n            \\n            # Define which columns should be ordinal-encoded and which should be scaled\\n            categorical_cols = [\\'cut\\', \\'color\\',\\'clarity\\']\\n            numerical_cols = [\\'carat\\', \\'depth\\',\\'table\\', \\'x\\', \\'y\\', \\'z\\']\\n            \\n            # Define the custom ranking for each ordinal variable\\n            cut_categories = [\\'Fair\\', \\'Good\\', \\'Very Good\\',\\'Premium\\',\\'Ideal\\']\\n            color_categories = [\\'D\\', \\'E\\', \\'F\\', \\'G\\', \\'H\\', \\'I\\', \\'J\\']\\n            clarity_categories = [\\'I1\\',\\'SI2\\',\\'SI1\\',\\'VS2\\',\\'VS1\\',\\'VVS2\\',\\'VVS1\\',\\'IF\\']\\n            \\n            logging.info(\\'Pipeline Initiated\\')\\n            \\n            ## Numerical Pipeline\\n            num_pipeline=Pipeline(\\n                steps=[\\n                (\\'imputer\\',SimpleImputer(strategy=\\'median\\')),\\n                (\\'scaler\\',StandardScaler())\\n\\n                ]\\n\\n            )\\n            \\n            # Categorigal Pipeline\\n            cat_pipeline=Pipeline(\\n                steps=[\\n                (\\'imputer\\',SimpleImputer(strategy=\\'most_frequent\\')),\\n                (\\'ordinalencoder\\',OrdinalEncoder(categories=[cut_categories,color_categories,clarity_categories])),\\n                (\\'scaler\\',StandardScaler())\\n                ]\\n\\n            )\\n            \\n            preprocessor=ColumnTransformer([\\n            (\\'num_pipeline\\',num_pipeline,numerical_cols),\\n            (\\'cat_pipeline\\',cat_pipeline,categorical_cols)\\n            ], remainder=\\'passthrough\\')\\n            \\n            return preprocessor\\n            \\n\\n            \\n            \\n        \\n        except Exception as e:\\n            logging.info(\"Exception occured in the initiate_datatransformation\")\\n\\n            raise CustomException(e,sys)\\n            \\n    \\n    def initialize_data_transformation(self,train_path,test_path):\\n        try:\\n            train_df=pd.read_csv(train_path)\\n            test_df=pd.read_csv(test_path)\\n            \\n            logging.info(\"read train and test data complete\")\\n            logging.info(f\\'Train Dataframe Head : \\\\n{train_df.head().to_string()}\\')\\n            logging.info(f\\'Test Dataframe Head : \\\\n{test_df.head().to_string()}\\')\\n            \\n            preprocessing_obj = self.get_data_transformation()\\n            \\n            target_column_name = \\'price\\'\\n            drop_columns = [target_column_name,\\'id\\']\\n            \\n            input_feature_train_df = train_df.drop(columns=drop_columns,axis=1)\\n            target_feature_train_df=train_df[target_column_name]\\n            \\n            \\n            input_feature_test_df=test_df.drop(columns=drop_columns,axis=1)\\n            target_feature_test_df=test_df[target_column_name]\\n            \\n            input_feature_train_arr=preprocessing_obj.fit_transform(input_feature_train_df)\\n            \\n            input_feature_test_arr=preprocessing_obj.transform(input_feature_test_df)\\n            \\n            logging.info(\"Applying preprocessing object on training and testing datasets.\")\\n            \\n            train_arr = np.c_[input_feature_train_arr, np.array(target_feature_train_df)]\\n            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)]\\n\\n            save_object(\\n                file_path=self.data_transformation_config.preprocessor_obj_file_path,\\n                obj=preprocessing_obj\\n            )\\n            \\n            logging.info(\"preprocessing pickle file saved\")\\n            \\n            return (\\n                train_arr,\\n                test_arr\\n            )\\n            \\n        except Exception as e:\\n            logging.info(\"Exception occured in the initiate_datatransformation\")\\n\\n            raise CustomException(e,sys)\\n            '),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nimport mlflow\\nimport mlflow.sklearn\\nimport numpy as np\\nimport pickle\\nfrom src.utils.utils import load_object\\nfrom urllib.parse import urlparse\\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\\nfrom src.logger.logger import logging\\nfrom src.exception. exception import CustomException\\n\\nclass ModelEvaluation:\\n    def __init__(self):\\n        logging.info(\"evaluation started\")\\n\\n    def eval_metrics(self,actual,pred):\\n        rmse = np.sqrt(mean_squared_error(actual, pred))# here is RMSE\\n        mae = mean_absolute_error(actual, pred)# here is MAE\\n        r2 = r2_score(actual, pred)# here is r3 value\\n        logging.info(\"evaluation metrics captured\")\\n        return rmse, mae, r2\\n\\n    def initiate_model_evaluation(self,train_array,test_array):\\n        try:\\n             X_test,y_test=(test_array[:,:-1], test_array[:,-1])\\n\\n             model_path=os.path.join(\"artifacts\",\"model.pkl\")\\n             model=load_object(model_path)\\n\\n             #mlflow.set_registry_uri(\"\")\\n             \\n             logging.info(\"model has register\")\\n\\n             tracking_url_type_store=urlparse(mlflow.get_tracking_uri()).scheme\\n\\n             print(tracking_url_type_store)\\n\\n\\n\\n             with mlflow.start_run():\\n\\n                prediction=model.predict(X_test)\\n\\n                (rmse,mae,r2)=self.eval_metrics(y_test,prediction)\\n\\n                mlflow.log_metric(\"rmse\", rmse)\\n                mlflow.log_metric(\"r2\", r2)\\n                mlflow.log_metric(\"mae\", mae)\\n\\n                 # Model registry does not work with file store\\n                if tracking_url_type_store != \"file\":\\n\\n                    # Register the model\\n                    # There are other ways to use the Model Registry, which depends on the use case,\\n                    # please refer to the doc for more information:\\n                    # https://mlflow.org/docs/latest/model-registry.html#api-workflow\\n                    mlflow.sklearn.log_model(model, \"model\", registered_model_name=\"ml_model\")\\n                else:\\n                    mlflow.sklearn.log_model(model, \"model\")\\n\\n\\n        except Exception as e:\\n            raise CustomException(e,sys)'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"import pandas as pd\\nimport numpy as np\\nfrom src.logger.logger import logging\\nfrom src.exception.exception import CustomException\\nimport os\\nimport sys\\nfrom dataclasses import dataclass\\nfrom pathlib import Path\\n\\nfrom src.utils.utils import save_object,evaluate_model\\n\\nfrom sklearn.linear_model import LinearRegression, Ridge,Lasso,ElasticNet\\n\\n\\n@dataclass \\nclass ModelTrainerConfig:\\n    trained_model_file_path = os.path.join('artifacts','model.pkl')\\n    \\n    \\nclass ModelTrainer:\\n    def __init__(self):\\n        self.model_trainer_config = ModelTrainerConfig()\\n    \\n    def initate_model_training(self,train_array,test_array):\\n        try:\\n            logging.info('Splitting Dependent and Independent variables from train and test data')\\n            X_train, y_train, X_test, y_test = (\\n                train_array[:,:-1],\\n                train_array[:,-1],\\n                test_array[:,:-1],\\n                test_array[:,-1]\\n            )\\n\\n            models={\\n            'LinearRegression':LinearRegression(),\\n            'Lasso':Lasso(),\\n            'Ridge':Ridge(),\\n            'Elasticnet':ElasticNet()\\n        }\\n            \\n            model_report:dict=evaluate_model(X_train,y_train,X_test,y_test,models)\\n            print(model_report)\\n            print('\\\\n====================================================================================\\\\n')\\n            logging.info(f'Model Report : {model_report}')\\n\\n            # To get best model score from dictionary \\n            best_model_score = max(sorted(model_report.values()))\\n\\n            best_model_name = list(model_report.keys())[\\n                list(model_report.values()).index(best_model_score)\\n            ]\\n            \\n            best_model = models[best_model_name]\\n\\n            print(f'Best Model Found , Model Name : {best_model_name} , R2 Score : {best_model_score}')\\n            print('\\\\n====================================================================================\\\\n')\\n            logging.info(f'Best Model Found , Model Name : {best_model_name} , R2 Score : {best_model_score}')\\n\\n            save_object(\\n                 file_path=self.model_trainer_config.trained_model_file_path,\\n                 obj=best_model\\n            )\\n          \\n\\n        except Exception as e:\\n            logging.info('Exception occured at Model Training')\\n            raise CustomException(e,sys)\\n\\n        \\n    \"),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\exception\\\\exception.py', 'language': <Language.PYTHON: 'python'>}, page_content='import sys\\n\\n\\n\\nclass CustomException(Exception):\\n    def __init__(self, error_message, error_detail:sys):\\n        self.error_message = error_message\\n        _, _, exc_tb = error_detail.exc_info()\\n\\n        self.lineno = exc_tb.tb_lineno\\n        self.filename = exc_tb.tb_frame.f_code.co_filename\\n\\n    def __str__(self):\\n        return f\"Error occured in python script name [{self.filename}] line number [{self.lineno}] error message [{self.error_message}]\"\\n    \\n\\n# if __name__ == \"__main__\":\\n#     try:\\n#         a=1/0\\n#     except Exception as e:\\n#         raise CustomException(e,sys)'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\exception\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\logger\\\\logger.py', 'language': <Language.PYTHON: 'python'>}, page_content='import logging\\nimport os\\nfrom datetime import datetime\\n\\nLOG_FILE = f\"{datetime.now().strftime(\\'%m_%d_%Y_%H_%M_%S\\')}.log\"\\nlog_path = os.path.join(os.getcwd(),\"logs\",LOG_FILE)\\nos.makedirs(log_path,exist_ok=True)\\n\\nLOG_FILE_PATH = os.path.join(log_path,LOG_FILE)\\n\\nlogging.basicConfig(\\n    filename=LOG_FILE_PATH,\\n    level=logging.INFO,\\n    format=\"[%(asctime)s] %(lineno)d %(name)s - %(levelname)s - %(message)s\"\\n)\\n\\n# if __name__ == \"__main__\":\\n#     logging.info(\"Logging has started\")'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\logger\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\pipeline\\\\batch_prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\pipeline\\\\prediction_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nimport pandas as pd\\nfrom src.exception.exception import CustomException\\nfrom src.logger.logger import logging\\nfrom src.utils.utils import load_object\\n\\n\\nclass PredictPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def predict(self, features):\\n        try:\\n            preprocessor_path = os.path.join(\"artifacts\", \"preprocessor.pkl\")\\n            model_path = os.path.join(\"artifacts\", \"model.pkl\")\\n\\n            preprocessor = load_object(file_path=preprocessor_path)\\n            model = load_object(file_path=model_path)\\n\\n            scaled_feature = preprocessor.transform(features)\\n\\n            pred = model.predict(scaled_feature)\\n\\n            return pred\\n\\n        except Exception as e:\\n            raise CustomException(e, sys)\\n        \\n\\nclass CustomData:\\n    def __init__(self,\\n                 carat:float,\\n                 depth:float,\\n                 table:float,\\n                 x:float,\\n                 y:float,\\n                 z:float,\\n                 cut:str,\\n                 color:str,\\n                 clarity:str):\\n        \\n        self.carat=carat\\n        self.depth=depth\\n        self.table=table\\n        self.x=x\\n        self.y=y\\n        self.z=z\\n        self.cut = cut\\n        self.color = color\\n        self.clarity = clarity\\n            \\n    def get_data_as_dataframe(self):\\n        try:\\n            custom_data_input_dict = {\\n                \\'carat\\':[self.carat],\\n                \\'depth\\':[self.depth],\\n                \\'table\\':[self.table],\\n                \\'x\\':[self.x],\\n                \\'y\\':[self.y],\\n                \\'z\\':[self.z],\\n                \\'cut\\':[self.cut],\\n                \\'color\\':[self.color],\\n                \\'clarity\\':[self.clarity]\\n                }\\n            df = pd.DataFrame(custom_data_input_dict)\\n            logging.info(\\'Dataframe Gathered\\')\\n            return df\\n        except Exception as e:\\n            logging.info(\\'Exception Occured in prediction pipeline\\')\\n            raise CustomException(e,sys)'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\pipeline\\\\training_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nfrom src.logger.logger import logging\\nfrom src.exception.exception import CustomException\\nimport pandas as pd\\n\\nfrom src.components.data_ingestion import DataIngestion\\nfrom src.components.data_transformation import DataTransformation\\nfrom src.components.model_trainer import ModelTrainer\\nfrom src.components.model_evaluation import ModelEvaluation\\n\\n\\n# obj=DataIngestion()\\n\\n# train_data_path,test_data_path=obj.initiate_data_ingestion()\\n\\n# data_transformation=DataTransformation()\\n\\n# train_arr,test_arr=data_transformation.initialize_data_transformation(train_data_path,test_data_path)\\n\\n\\n# model_trainer_obj=ModelTrainer()\\n# model_trainer_obj.initate_model_training(train_arr,test_arr)\\n\\n# model_eval_obj = ModelEvaluation()\\n# model_eval_obj.initiate_model_evaluation(train_arr,test_arr)\\n\\n\\nclass TrainingPipeline:\\n    def start_data_ingestion(self):\\n        try:\\n            data_ingestion=DataIngestion()\\n            train_data_path,test_data_path=data_ingestion.initiate_data_ingestion()\\n            return train_data_path,test_data_path\\n        except Exception as e:\\n            raise CustomException(e, sys)\\n\\n    def start_data_transformation(self,train_data_path,test_data_path):\\n        try:\\n            data_transformation=DataTransformation()\\n\\n            train_arr,test_arr=data_transformation.initialize_data_transformation(train_data_path,test_data_path)\\n            return train_arr,test_arr\\n        except Exception as e:\\n            raise CustomException(e, sys)\\n\\n    def start_model_trainer(self, train_arr,test_arr):\\n        try:\\n            model_trainer_obj=ModelTrainer()\\n            model_trainer_obj.initate_model_training(train_arr,test_arr)\\n        except Exception as e:\\n            raise CustomException(e, sys)\\n        \\n    def start_model_evaluation(self, train_arr,test_arr):\\n        try:\\n            model_eval_obj = ModelEvaluation()\\n            model_eval_obj.initiate_model_evaluation(train_arr,test_arr)\\n        except Exception as e:\\n            raise CustomException(e, sys)\\n        \\n    def start_pipeline(self):\\n        try:\\n            train_data_path,test_data_path=self.start_data_ingestion()\\n            train_arr,test_arr=self.start_data_transformation(train_data_path,test_data_path)\\n            self.start_model_trainer(train_arr,test_arr)\\n            self.start_model_evaluation(train_arr,test_arr)\\n        except Exception as e:\\n            raise CustomException(e, sys)'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\pipeline\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\utils\\\\utils.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nimport pickle\\nimport numpy as np\\nimport pandas as pd\\nfrom src.logger.logger import logging\\nfrom src.exception.exception import CustomException\\n\\nfrom sklearn.metrics import r2_score, mean_absolute_error,mean_squared_error\\n\\ndef save_object(file_path, obj):\\n    try:\\n        dir_path = os.path.dirname(file_path)\\n\\n        os.makedirs(dir_path, exist_ok=True)\\n\\n        with open(file_path, \"wb\") as file_obj:\\n            pickle.dump(obj, file_obj)\\n\\n    except Exception as e:\\n        raise CustomException(e, sys)\\n    \\ndef evaluate_model(X_train,y_train,X_test,y_test,models):\\n    try:\\n        report = {}\\n        for i in range(len(models)):\\n            model = list(models.values())[i]\\n            # Train model\\n            model.fit(X_train,y_train)\\n\\n            # Predict Testing data\\n            y_test_pred =model.predict(X_test)\\n\\n            # Get R2 scores for train and test data\\n            #train_model_score = r2_score(ytrain,y_train_pred)\\n            test_model_score = r2_score(y_test,y_test_pred)\\n\\n            report[list(models.keys())[i]] =  test_model_score\\n\\n        return report\\n\\n    except Exception as e:\\n        logging.info(\\'Exception occured during model training\\')\\n        raise CustomException(e,sys)\\n    \\ndef load_object(file_path):\\n    try:\\n        with open(file_path,\\'rb\\') as file_obj:\\n            return pickle.load(file_obj)\\n    except Exception as e:\\n        logging.info(\\'Exception Occured in load_object function utils\\')\\n        raise CustomException(e,sys)\\n\\n    '),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\utils\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'test_dir\\\\tests\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'test_dir\\\\tests\\\\integration\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content=''),\n",
       " Document(metadata={'source': 'test_dir\\\\tests\\\\unit\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'test_dir\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, render_template, request\\nfrom src.pipeline.prediction_pipeline import PredictPipeline, CustomData\\nfrom src.exception.exception import CustomException\\nfrom src.logger.logger import logging\\nimport sys\\n\\napp=Flask(__name__)\\n\\n@app.route(\"/\")\\ndef home_page():\\n    try:\\n        return render_template(\\'index.html\\')\\n    except Exception as e:\\n        raise CustomException(e,sys)\\n    \\n@app.route(\"/predict\", methods=[\\'POST\\', \"GET\"])\\ndef predict_datapoint():\\n    try:\\n        if request.method==\"GET\":\\n            return render_template(\\'form.html\\')\\n        else:\\n            data=CustomData(\\n                carat=float(request.form.get(\"carat\")),\\n                depth=float(request.form.get(\"depth\")),\\n                table=float(request.form.get(\"table\")),\\n                x=float(request.form.get(\"x\")),\\n                y=float(request.form.get(\"y\")),\\n                z=float(request.form.get(\"z\")),\\n                cut=request.form.get(\"cut\"),\\n                color=request.form.get(\"color\"),\\n                clarity=request.form.get(\"clarity\")\\n            )\\n\\n            final_data = data.get_data_as_dataframe()\\n            pred_pipeline=PredictPipeline()\\n            pred=pred_pipeline.predict(final_data)\\n\\n            result = round(pred[0],2)\\n            return render_template(\\'result.html\\', final_result=result)\\n\\n    except Exception as e:\\n        raise CustomException(e,sys)\\n    \\nif __name__==\"__main__\":\\n    app.run(host=\"0.0.0.0\", port=80, debug=True)')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language = Language.PYTHON,\n",
    "                                                             chunk_size = 500,\n",
    "                                                             chunk_overlap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_dir\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, render_template, request\\nfrom src.pipeline.prediction_pipeline import PredictPipeline, CustomData\\nfrom src.exception.exception import CustomException\\nfrom src.logger.logger import logging\\nimport sys\\n\\napp=Flask(__name__)\\n\\n@app.route(\"/\")\\ndef home_page():\\n    try:\\n        return render_template(\\'index.html\\')\\n    except Exception as e:\\n        raise CustomException(e,sys)\\n    \\n@app.route(\"/predict\", methods=[\\'POST\\', \"GET\"])'),\n",
       " Document(metadata={'source': 'test_dir\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='def predict_datapoint():\\n    try:\\n        if request.method==\"GET\":\\n            return render_template(\\'form.html\\')\\n        else:\\n            data=CustomData(\\n                carat=float(request.form.get(\"carat\")),\\n                depth=float(request.form.get(\"depth\")),\\n                table=float(request.form.get(\"table\")),\\n                x=float(request.form.get(\"x\")),\\n                y=float(request.form.get(\"y\")),\\n                z=float(request.form.get(\"z\")),'),\n",
       " Document(metadata={'source': 'test_dir\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='cut=request.form.get(\"cut\"),\\n                color=request.form.get(\"color\"),\\n                clarity=request.form.get(\"clarity\")\\n            )'),\n",
       " Document(metadata={'source': 'test_dir\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='final_data = data.get_data_as_dataframe()\\n            pred_pipeline=PredictPipeline()\\n            pred=pred_pipeline.predict(final_data)\\n\\n            result = round(pred[0],2)\\n            return render_template(\\'result.html\\', final_result=result)\\n\\n    except Exception as e:\\n        raise CustomException(e,sys)\\n    \\nif __name__==\"__main__\":\\n    app.run(host=\"0.0.0.0\", port=80, debug=True)'),\n",
       " Document(metadata={'source': 'test_dir\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content='from setuptools import find_packages,setup\\nfrom typing import List\\n\\n\\nHYPHEN_E_DOT = \"-e .\"'),\n",
       " Document(metadata={'source': 'test_dir\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content='def get_requirements(filepath: str) -> List[str]:\\n    requirements = []\\n    with open(filepath) as file_obj:\\n        requirements = file_obj.readlines()\\n        requirements = [req.replace(\"\\\\n\", \"\") for req in requirements]\\n\\n        if HYPHEN_E_DOT in requirements:\\n            requirements.remove(HYPHEN_E_DOT)\\n\\n    return requirements'),\n",
       " Document(metadata={'source': 'test_dir\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content='setup(\\n    name=\\'GemPricePrediction\\',\\n    version=\\'0.0.1\\',\\n    author=\\'Suryansh Pandey\\',\\n    author_email=\\'suryanshp1@gmail.com\\',\\n    install_requires=get_requirements(\"requirements.txt\"),\\n    packages=find_packages()\\n)'),\n",
       " Document(metadata={'source': 'test_dir\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom pathlib import Path'),\n",
       " Document(metadata={'source': 'test_dir\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='list_of_files = [\\n    \".github/workflows/.gitkeep\",\\n    \"src/__init__.py\",\\n    \"src/components/__init__.py\",\\n    \"src/components/data_ingestion.py\",\\n    \"src/components/data_transformation.py\",\\n    \"src/components/model_trainer.py\",\\n    \"src/components/model_evaluation.py\",\\n    \"src/pipeline/__init__.py\",\\n    \"src/pipeline/training_pipeline.py\",\\n    \"src/pipeline/prediction_pipeline.py\",\\n    \"src/utils/__init__.py\",\\n    \"src/utils/utils.py\",\\n    \"src/logger/__init__.py\",'),\n",
       " Document(metadata={'source': 'test_dir\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='\"src/logger/logger.py\",\\n    \"src/exception/__init__.py\",\\n    \"src/exception/exception.py\",\\n    \"tests/__init__.py\",\\n    \"tests/unit/__init__.py\",\\n    \"tests/integration/__init__.py\",\\n    \"init_setup.sh\",\\n    \"requirements.txt\",\\n    \"requirements_dev.txt\",\\n    \"setup.py\",\\n    \"pyproject.toml\",\\n    \"setup.cfg\",\\n    \"tox.ini\",\\n    \"experiment/experiments.ipynb\"\\n]'),\n",
       " Document(metadata={'source': 'test_dir\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='for filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)\\n\\n    if filedir != \"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        # logging.info(f\"Creating directory: {filedir} for file: {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass'),\n",
       " Document(metadata={'source': 'test_dir\\\\airflow\\\\dags\\\\batch_prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content='\"\"\"from asyncio import tasks\\nimport json\\nfrom textwrap import dedent\\nimport pendulum\\nimport os\\nfrom airflow import DAG\\nfrom airflow.operators.python import PythonOperator'),\n",
       " Document(metadata={'source': 'test_dir\\\\airflow\\\\dags\\\\batch_prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content='with DAG(\\n    \\'batch_prediction\\',\\n    default_args={\\'retries\\': 2},\\n    # [END default_args]\\n    description=\\'gemstone batch prediction\\',\\n    schedule_interval=\"@weekly\", # here you can test based on hour or mints but make sure here you container is up and running\\n    start_date=pendulum.datetime(2023,4, 11, tz=\"UTC\"),\\n    catchup=False,\\n    tags=[\\'example\\'],\\n) as dag:\\n    def download_files(**kwargs):\\n        bucket_name = os.getenv(\"BUCKET_NAME\")# download the file from the repo'),\n",
       " Document(metadata={'source': 'test_dir\\\\airflow\\\\dags\\\\batch_prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content='input_dir = \"/app/input_files\"\\n        #creating directory\\n        os.makedirs(input_dir,exist_ok=True)\\n        #os.system(f\"aws s3 sync s3://{bucket_name}/inbox {config.inbox_dir}\")'),\n",
       " Document(metadata={'source': 'test_dir\\\\airflow\\\\dags\\\\batch_prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content='def batch_prediction(**kwargs):\\n        config = BatchPredictionConfig()\\n        sensor_batch_prediction = SensorBatchPrediction(batch_config=config)\\n        sensor_batch_prediction.start_prediction()\\n       \\n    def upload_files(**kwargs):\\n        #bucket_name = os.getenv(\"BUCKET_NAME\")\\n        #os.system(f\"aws s3 sync {config.archive_dir} s3://{bucket_name}/archive\")\\n        #os.system(f\"aws s3 sync {config.outbox_dir} s3://{bucket_name}/outbox\")'),\n",
       " Document(metadata={'source': 'test_dir\\\\airflow\\\\dags\\\\batch_prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content='# download_input_files  = PythonOperator(\\n            task_id=\"download_file\",\\n            python_callable=download_files\\n\\n    )\\n\\n    generate_prediction_files = PythonOperator(\\n            task_id=\"prediction\",\\n            python_callable=batch_prediction\\n\\n    )\\n\\n    upload_prediction_files = PythonOperator(\\n            task_id=\"upload_prediction_files\",\\n            python_callable=upload_files\\n\\n    )'),\n",
       " Document(metadata={'source': 'test_dir\\\\airflow\\\\dags\\\\batch_prediction.py', 'language': <Language.PYTHON: 'python'>}, page_content=')\\n\\n    download_input_files >> generate_prediction_files >> upload_prediction_files\\n    \\n    \"\"\"'),\n",
       " Document(metadata={'source': 'test_dir\\\\airflow\\\\dags\\\\training_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='from __future__ import annotations\\nfrom textwrap import dedent\\nimport pendulum\\nimport subprocess\\nfrom airflow import DAG\\nfrom airflow.operators.python import PythonOperator\\nfrom src.pipeline.training_pipeline import TrainingPipeline\\n\\n\\ntraining_pipeline = TrainingPipeline()'),\n",
       " Document(metadata={'source': 'test_dir\\\\airflow\\\\dags\\\\training_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='with DAG(\\n    dag_id=\"gemstone_training_pipeline\",\\n    default_args={\\n        \"retries\": 3,\\n    },\\n    description=\"Training pipeline for gemstone dataset\",\\n    schedule=\"@weekly\",\\n    start_date=pendulum.datetime(2024, 10, 6, tz=\"UTC\"),\\n    catchup=False,\\n    tags=[\"training\", \"machine_learning\", \"classification\"],\\n) as dag:\\n    \\n    dag.doc_md = __doc__'),\n",
       " Document(metadata={'source': 'test_dir\\\\airflow\\\\dags\\\\training_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='def data_ingestion(**kwargs):\\n        ti = kwargs.get(\"ti\")\\n        train_data_path, test_data_path = training_pipeline.start_data_ingestion()\\n        ti.xcom_push(key=\"data_ingestion_artifact\", value={\"train_data_path\": train_data_path, \"test_data_path\": test_data_path})'),\n",
       " Document(metadata={'source': 'test_dir\\\\airflow\\\\dags\\\\training_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='def data_transformation(**kwargs):\\n        ti = kwargs.get(\"ti\")\\n        data_ingestion_artifact = ti.xcom_pull(key=\"data_ingestion_artifact\", task_ids=\"data_ingestion\")\\n        train_arr, test_arr = training_pipeline.start_data_transformation(data_ingestion_artifact.get(\"train_data_path\"), data_ingestion_artifact.get(\"test_data_path\"))\\n        train_arr=train_arr.tolist()\\n        test_arr=test_arr.tolist()'),\n",
       " Document(metadata={'source': 'test_dir\\\\airflow\\\\dags\\\\training_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='ti.xcom_push(key=\"data_transformation_artifact\", value={\"train_arr\": train_arr, \"test_arr\": test_arr})'),\n",
       " Document(metadata={'source': 'test_dir\\\\airflow\\\\dags\\\\training_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='def model_trainer(**kwargs):\\n        import numpy as np\\n\\n        ti = kwargs.get(\"ti\")\\n        data_transformation_artifact = ti.xcom_pull(key=\"data_transformation_artifact\", task_ids=\"data_transformation\")\\n\\n        train_arr = np.array(data_transformation_artifact.get(\"train_arr\"))\\n        test_arr = np.array(data_transformation_artifact.get(\"test_arr\"))\\n\\n        training_pipeline.start_model_trainer(train_arr, test_arr)'),\n",
       " Document(metadata={'source': 'test_dir\\\\airflow\\\\dags\\\\training_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='def push_data_to_azureblob(**kwargs):\\n        # first configure azure cli in environment\\n        account_name = \"<account-name>\"\\n        destination_path = \"<destination-path>\"\\n        source_path = \"/app/artifacts\"\\n        # save it to the azure blob\\n        subprocess.run(f\"az storage blob upload-batch --account-name {account_name} --destination-path {destination_path} --source {source_path}\", shell=False)'),\n",
       " Document(metadata={'source': 'test_dir\\\\airflow\\\\dags\\\\training_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='data_ingestion_task = PythonOperator(\\n        task_id=\"data_ingestion\",\\n        python_callable=data_ingestion,\\n    )\\n    data_ingestion_task.doc_md = dedent(\\n        \"\"\"\\\\\\n    #### Ingestion task\\n    this task creates a train and test file.\\n    \"\"\"\\n    )'),\n",
       " Document(metadata={'source': 'test_dir\\\\airflow\\\\dags\\\\training_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='data_transformation_task = PythonOperator(\\n        task_id=\"data_transformation\",\\n        python_callable=data_transformation,\\n    )\\n    data_transformation_task.doc_md = dedent(\\n        \"\"\"\\\\\\n    #### Transformation task\\n    this task performs the transformation\\n    \"\"\"\\n    )'),\n",
       " Document(metadata={'source': 'test_dir\\\\airflow\\\\dags\\\\training_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='model_trainer_task = PythonOperator(\\n        task_id=\"model_trainer\",\\n        python_callable=model_trainer,\\n    )\\n    model_trainer_task.doc_md = dedent(\\n        \"\"\"\\\\\\n    #### model trainer task\\n    this task perform training\\n    \"\"\"\\n    )\\n\\n\\n    # push_data_to_azureblob_task = PythonOperator(\\n    #     task_id=\"push_data_to_azureblob\",\\n    #     python_callable=push_data_to_azureblob,\\n    # )'),\n",
       " Document(metadata={'source': 'test_dir\\\\airflow\\\\dags\\\\training_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='# push_data_to_azureblob_task.doc_md = dedent(\\n    #     \"\"\"\\\\\\n    # #### push data to azureblob task\\n    # this task push data to azureblob\\n    # \"\"\"\\n    # )\\n\\n    data_ingestion_task >> data_transformation_task >> model_trainer_task'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='import pandas as pd\\nimport numpy as np\\nfrom src.logger.logger import logging\\nfrom src.exception.exception import CustomException\\n\\nimport os\\nimport sys\\nfrom sklearn.model_selection import train_test_split\\nfrom dataclasses import dataclass\\nfrom pathlib import Path\\n\\n@dataclass\\nclass DataIngestionConfig:\\n    raw_data_path = os.path.join(\"artifacts\", \"raw.csv\")\\n    train_data_path = os.path.join(\"artifacts\", \"train.csv\")\\n    test_data_path = os.path.join(\"artifacts\", \"test.csv\")'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='class DataIngestion:\\n    def __init__(self):\\n        self.ingestion_config = DataIngestionConfig()\\n\\n    def initiate_data_ingestion(self):\\n        logging.info(\"Started the data ingestion.....\")\\n        try:\\n            logging.info(\"Entered the data ingestion method or component\")\\n            data = pd.read_csv(\"https://raw.githubusercontent.com/suryanshp1/Gem-Price-Prediction-end-to-end-pipeline/refs/heads/main/data/train.csv\")\\n            logging.info(\"Reading the dataset as dataframe\")'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='os.makedirs(os.path.dirname(self.ingestion_config.raw_data_path), exist_ok=True)\\n            data.to_csv(self.ingestion_config.raw_data_path, index=False)\\n            logging.info(\"Saved the raw data in artifact folder\")\\n            logging.info(\"Train test split initiated\")\\n\\n            train_data, test_data = train_test_split(data, test_size=0.25)\\n\\n            logging.info(\"Train test split completed\")'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='train_data.to_csv(self.ingestion_config.train_data_path, index=False)\\n            test_data.to_csv(self.ingestion_config.test_data_path, index=False)\\n\\n            logging.info(\"Data ingestion of data completed\")\\n\\n            return (\\n                self.ingestion_config.train_data_path,\\n                self.ingestion_config.test_data_path,\\n            )\\n\\n        except Exception as e:\\n            logging.info(e)\\n            raise CustomException(e, sys)'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\data_ingestion.py', 'language': <Language.PYTHON: 'python'>}, page_content='# if __name__ == \"__main__\":\\n#     obj = DataIngestion()\\n#     train_data, test_data = obj.initiate_data_ingestion()'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='import pandas as pd\\nimport numpy as np\\nfrom src.logger.logger import logging\\nfrom src.exception.exception import CustomException\\nimport os\\nimport sys\\nfrom dataclasses import dataclass\\nfrom pathlib import Path\\n\\n\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import OrdinalEncoder,StandardScaler\\n\\nfrom src.utils.utils import save_object\\n\\n@dataclass'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"class DataTransformationConfig:\\n    preprocessor_obj_file_path=os.path.join('artifacts','preprocessor.pkl')\"),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='class DataTransformation:\\n    def __init__(self):\\n        self.data_transformation_config=DataTransformationConfig()'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"def get_data_transformation(self):\\n        \\n        try:\\n            logging.info('Data Transformation initiated')\\n            \\n            # Define which columns should be ordinal-encoded and which should be scaled\\n            categorical_cols = ['cut', 'color','clarity']\\n            numerical_cols = ['carat', 'depth','table', 'x', 'y', 'z']\\n            \\n            # Define the custom ranking for each ordinal variable\"),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"cut_categories = ['Fair', 'Good', 'Very Good','Premium','Ideal']\\n            color_categories = ['D', 'E', 'F', 'G', 'H', 'I', 'J']\\n            clarity_categories = ['I1','SI2','SI1','VS2','VS1','VVS2','VVS1','IF']\\n            \\n            logging.info('Pipeline Initiated')\\n            \\n            ## Numerical Pipeline\\n            num_pipeline=Pipeline(\\n                steps=[\\n                ('imputer',SimpleImputer(strategy='median')),\\n                ('scaler',StandardScaler())\"),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"]\\n\\n            )\\n            \\n            # Categorigal Pipeline\\n            cat_pipeline=Pipeline(\\n                steps=[\\n                ('imputer',SimpleImputer(strategy='most_frequent')),\\n                ('ordinalencoder',OrdinalEncoder(categories=[cut_categories,color_categories,clarity_categories])),\\n                ('scaler',StandardScaler())\\n                ]\"),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content=')\\n            \\n            preprocessor=ColumnTransformer([\\n            (\\'num_pipeline\\',num_pipeline,numerical_cols),\\n            (\\'cat_pipeline\\',cat_pipeline,categorical_cols)\\n            ], remainder=\\'passthrough\\')\\n            \\n            return preprocessor\\n            \\n\\n            \\n            \\n        \\n        except Exception as e:\\n            logging.info(\"Exception occured in the initiate_datatransformation\")'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='raise CustomException(e,sys)\\n            \\n    \\n    def initialize_data_transformation(self,train_path,test_path):\\n        try:\\n            train_df=pd.read_csv(train_path)\\n            test_df=pd.read_csv(test_path)\\n            \\n            logging.info(\"read train and test data complete\")\\n            logging.info(f\\'Train Dataframe Head : \\\\n{train_df.head().to_string()}\\')\\n            logging.info(f\\'Test Dataframe Head : \\\\n{test_df.head().to_string()}\\')'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"preprocessing_obj = self.get_data_transformation()\\n            \\n            target_column_name = 'price'\\n            drop_columns = [target_column_name,'id']\\n            \\n            input_feature_train_df = train_df.drop(columns=drop_columns,axis=1)\\n            target_feature_train_df=train_df[target_column_name]\\n            \\n            \\n            input_feature_test_df=test_df.drop(columns=drop_columns,axis=1)\"),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='target_feature_test_df=test_df[target_column_name]\\n            \\n            input_feature_train_arr=preprocessing_obj.fit_transform(input_feature_train_df)\\n            \\n            input_feature_test_arr=preprocessing_obj.transform(input_feature_test_df)\\n            \\n            logging.info(\"Applying preprocessing object on training and testing datasets.\")\\n            \\n            train_arr = np.c_[input_feature_train_arr, np.array(target_feature_train_df)]'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)]'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='save_object(\\n                file_path=self.data_transformation_config.preprocessor_obj_file_path,\\n                obj=preprocessing_obj\\n            )\\n            \\n            logging.info(\"preprocessing pickle file saved\")\\n            \\n            return (\\n                train_arr,\\n                test_arr\\n            )\\n            \\n        except Exception as e:\\n            logging.info(\"Exception occured in the initiate_datatransformation\")'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\data_transformation.py', 'language': <Language.PYTHON: 'python'>}, page_content='raise CustomException(e,sys)'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nimport mlflow\\nimport mlflow.sklearn\\nimport numpy as np\\nimport pickle\\nfrom src.utils.utils import load_object\\nfrom urllib.parse import urlparse\\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score\\nfrom src.logger.logger import logging\\nfrom src.exception. exception import CustomException'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='class ModelEvaluation:\\n    def __init__(self):\\n        logging.info(\"evaluation started\")\\n\\n    def eval_metrics(self,actual,pred):\\n        rmse = np.sqrt(mean_squared_error(actual, pred))# here is RMSE\\n        mae = mean_absolute_error(actual, pred)# here is MAE\\n        r2 = r2_score(actual, pred)# here is r3 value\\n        logging.info(\"evaluation metrics captured\")\\n        return rmse, mae, r2'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='def initiate_model_evaluation(self,train_array,test_array):\\n        try:\\n             X_test,y_test=(test_array[:,:-1], test_array[:,-1])\\n\\n             model_path=os.path.join(\"artifacts\",\"model.pkl\")\\n             model=load_object(model_path)\\n\\n             #mlflow.set_registry_uri(\"\")\\n             \\n             logging.info(\"model has register\")\\n\\n             tracking_url_type_store=urlparse(mlflow.get_tracking_uri()).scheme\\n\\n             print(tracking_url_type_store)'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='with mlflow.start_run():\\n\\n                prediction=model.predict(X_test)\\n\\n                (rmse,mae,r2)=self.eval_metrics(y_test,prediction)\\n\\n                mlflow.log_metric(\"rmse\", rmse)\\n                mlflow.log_metric(\"r2\", r2)\\n                mlflow.log_metric(\"mae\", mae)\\n\\n                 # Model registry does not work with file store\\n                if tracking_url_type_store != \"file\":'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='# Register the model\\n                    # There are other ways to use the Model Registry, which depends on the use case,\\n                    # please refer to the doc for more information:\\n                    # https://mlflow.org/docs/latest/model-registry.html#api-workflow\\n                    mlflow.sklearn.log_model(model, \"model\", registered_model_name=\"ml_model\")\\n                else:\\n                    mlflow.sklearn.log_model(model, \"model\")'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\model_evaluation.py', 'language': <Language.PYTHON: 'python'>}, page_content='except Exception as e:\\n            raise CustomException(e,sys)'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"import pandas as pd\\nimport numpy as np\\nfrom src.logger.logger import logging\\nfrom src.exception.exception import CustomException\\nimport os\\nimport sys\\nfrom dataclasses import dataclass\\nfrom pathlib import Path\\n\\nfrom src.utils.utils import save_object,evaluate_model\\n\\nfrom sklearn.linear_model import LinearRegression, Ridge,Lasso,ElasticNet\\n\\n\\n@dataclass \\nclass ModelTrainerConfig:\\n    trained_model_file_path = os.path.join('artifacts','model.pkl')\"),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"class ModelTrainer:\\n    def __init__(self):\\n        self.model_trainer_config = ModelTrainerConfig()\\n    \\n    def initate_model_training(self,train_array,test_array):\\n        try:\\n            logging.info('Splitting Dependent and Independent variables from train and test data')\\n            X_train, y_train, X_test, y_test = (\\n                train_array[:,:-1],\\n                train_array[:,-1],\\n                test_array[:,:-1],\\n                test_array[:,-1]\\n            )\"),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"models={\\n            'LinearRegression':LinearRegression(),\\n            'Lasso':Lasso(),\\n            'Ridge':Ridge(),\\n            'Elasticnet':ElasticNet()\\n        }\\n            \\n            model_report:dict=evaluate_model(X_train,y_train,X_test,y_test,models)\\n            print(model_report)\\n            print('\\\\n====================================================================================\\\\n')\\n            logging.info(f'Model Report : {model_report}')\"),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content='# To get best model score from dictionary \\n            best_model_score = max(sorted(model_report.values()))\\n\\n            best_model_name = list(model_report.keys())[\\n                list(model_report.values()).index(best_model_score)\\n            ]\\n            \\n            best_model = models[best_model_name]'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"print(f'Best Model Found , Model Name : {best_model_name} , R2 Score : {best_model_score}')\\n            print('\\\\n====================================================================================\\\\n')\\n            logging.info(f'Best Model Found , Model Name : {best_model_name} , R2 Score : {best_model_score}')\\n\\n            save_object(\\n                 file_path=self.model_trainer_config.trained_model_file_path,\\n                 obj=best_model\\n            )\"),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\components\\\\model_trainer.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"except Exception as e:\\n            logging.info('Exception occured at Model Training')\\n            raise CustomException(e,sys)\"),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\exception\\\\exception.py', 'language': <Language.PYTHON: 'python'>}, page_content='import sys'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\exception\\\\exception.py', 'language': <Language.PYTHON: 'python'>}, page_content='class CustomException(Exception):\\n    def __init__(self, error_message, error_detail:sys):\\n        self.error_message = error_message\\n        _, _, exc_tb = error_detail.exc_info()\\n\\n        self.lineno = exc_tb.tb_lineno\\n        self.filename = exc_tb.tb_frame.f_code.co_filename\\n\\n    def __str__(self):\\n        return f\"Error occured in python script name [{self.filename}] line number [{self.lineno}] error message [{self.error_message}]\"'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\exception\\\\exception.py', 'language': <Language.PYTHON: 'python'>}, page_content='# if __name__ == \"__main__\":\\n#     try:\\n#         a=1/0\\n#     except Exception as e:\\n#         raise CustomException(e,sys)'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\logger\\\\logger.py', 'language': <Language.PYTHON: 'python'>}, page_content='import logging\\nimport os\\nfrom datetime import datetime\\n\\nLOG_FILE = f\"{datetime.now().strftime(\\'%m_%d_%Y_%H_%M_%S\\')}.log\"\\nlog_path = os.path.join(os.getcwd(),\"logs\",LOG_FILE)\\nos.makedirs(log_path,exist_ok=True)\\n\\nLOG_FILE_PATH = os.path.join(log_path,LOG_FILE)\\n\\nlogging.basicConfig(\\n    filename=LOG_FILE_PATH,\\n    level=logging.INFO,\\n    format=\"[%(asctime)s] %(lineno)d %(name)s - %(levelname)s - %(message)s\"\\n)\\n\\n# if __name__ == \"__main__\":\\n#     logging.info(\"Logging has started\")'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\pipeline\\\\prediction_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nimport pandas as pd\\nfrom src.exception.exception import CustomException\\nfrom src.logger.logger import logging\\nfrom src.utils.utils import load_object'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\pipeline\\\\prediction_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='class PredictPipeline:\\n    def __init__(self):\\n        pass\\n\\n    def predict(self, features):\\n        try:\\n            preprocessor_path = os.path.join(\"artifacts\", \"preprocessor.pkl\")\\n            model_path = os.path.join(\"artifacts\", \"model.pkl\")\\n\\n            preprocessor = load_object(file_path=preprocessor_path)\\n            model = load_object(file_path=model_path)\\n\\n            scaled_feature = preprocessor.transform(features)\\n\\n            pred = model.predict(scaled_feature)'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\pipeline\\\\prediction_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='return pred\\n\\n        except Exception as e:\\n            raise CustomException(e, sys)'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\pipeline\\\\prediction_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='class CustomData:\\n    def __init__(self,\\n                 carat:float,\\n                 depth:float,\\n                 table:float,\\n                 x:float,\\n                 y:float,\\n                 z:float,\\n                 cut:str,\\n                 color:str,\\n                 clarity:str):\\n        \\n        self.carat=carat\\n        self.depth=depth\\n        self.table=table\\n        self.x=x\\n        self.y=y\\n        self.z=z\\n        self.cut = cut\\n        self.color = color'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\pipeline\\\\prediction_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"self.clarity = clarity\\n            \\n    def get_data_as_dataframe(self):\\n        try:\\n            custom_data_input_dict = {\\n                'carat':[self.carat],\\n                'depth':[self.depth],\\n                'table':[self.table],\\n                'x':[self.x],\\n                'y':[self.y],\\n                'z':[self.z],\\n                'cut':[self.cut],\\n                'color':[self.color],\\n                'clarity':[self.clarity]\\n                }\"),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\pipeline\\\\prediction_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"}\\n            df = pd.DataFrame(custom_data_input_dict)\\n            logging.info('Dataframe Gathered')\\n            return df\\n        except Exception as e:\\n            logging.info('Exception Occured in prediction pipeline')\\n            raise CustomException(e,sys)\"),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\pipeline\\\\training_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nfrom src.logger.logger import logging\\nfrom src.exception.exception import CustomException\\nimport pandas as pd\\n\\nfrom src.components.data_ingestion import DataIngestion\\nfrom src.components.data_transformation import DataTransformation\\nfrom src.components.model_trainer import ModelTrainer\\nfrom src.components.model_evaluation import ModelEvaluation\\n\\n\\n# obj=DataIngestion()\\n\\n# train_data_path,test_data_path=obj.initiate_data_ingestion()\\n\\n# data_transformation=DataTransformation()'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\pipeline\\\\training_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='# train_arr,test_arr=data_transformation.initialize_data_transformation(train_data_path,test_data_path)\\n\\n\\n# model_trainer_obj=ModelTrainer()\\n# model_trainer_obj.initate_model_training(train_arr,test_arr)\\n\\n# model_eval_obj = ModelEvaluation()\\n# model_eval_obj.initiate_model_evaluation(train_arr,test_arr)'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\pipeline\\\\training_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='class TrainingPipeline:\\n    def start_data_ingestion(self):\\n        try:\\n            data_ingestion=DataIngestion()\\n            train_data_path,test_data_path=data_ingestion.initiate_data_ingestion()\\n            return train_data_path,test_data_path\\n        except Exception as e:\\n            raise CustomException(e, sys)\\n\\n    def start_data_transformation(self,train_data_path,test_data_path):\\n        try:\\n            data_transformation=DataTransformation()'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\pipeline\\\\training_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='train_arr,test_arr=data_transformation.initialize_data_transformation(train_data_path,test_data_path)\\n            return train_arr,test_arr\\n        except Exception as e:\\n            raise CustomException(e, sys)'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\pipeline\\\\training_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='def start_model_trainer(self, train_arr,test_arr):\\n        try:\\n            model_trainer_obj=ModelTrainer()\\n            model_trainer_obj.initate_model_training(train_arr,test_arr)\\n        except Exception as e:\\n            raise CustomException(e, sys)\\n        \\n    def start_model_evaluation(self, train_arr,test_arr):\\n        try:\\n            model_eval_obj = ModelEvaluation()\\n            model_eval_obj.initiate_model_evaluation(train_arr,test_arr)\\n        except Exception as e:'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\pipeline\\\\training_pipeline.py', 'language': <Language.PYTHON: 'python'>}, page_content='raise CustomException(e, sys)\\n        \\n    def start_pipeline(self):\\n        try:\\n            train_data_path,test_data_path=self.start_data_ingestion()\\n            train_arr,test_arr=self.start_data_transformation(train_data_path,test_data_path)\\n            self.start_model_trainer(train_arr,test_arr)\\n            self.start_model_evaluation(train_arr,test_arr)\\n        except Exception as e:\\n            raise CustomException(e, sys)'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\utils\\\\utils.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nimport sys\\nimport pickle\\nimport numpy as np\\nimport pandas as pd\\nfrom src.logger.logger import logging\\nfrom src.exception.exception import CustomException\\n\\nfrom sklearn.metrics import r2_score, mean_absolute_error,mean_squared_error'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\utils\\\\utils.py', 'language': <Language.PYTHON: 'python'>}, page_content='def save_object(file_path, obj):\\n    try:\\n        dir_path = os.path.dirname(file_path)\\n\\n        os.makedirs(dir_path, exist_ok=True)\\n\\n        with open(file_path, \"wb\") as file_obj:\\n            pickle.dump(obj, file_obj)\\n\\n    except Exception as e:\\n        raise CustomException(e, sys)'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\utils\\\\utils.py', 'language': <Language.PYTHON: 'python'>}, page_content='def evaluate_model(X_train,y_train,X_test,y_test,models):\\n    try:\\n        report = {}\\n        for i in range(len(models)):\\n            model = list(models.values())[i]\\n            # Train model\\n            model.fit(X_train,y_train)\\n\\n            # Predict Testing data\\n            y_test_pred =model.predict(X_test)\\n\\n            # Get R2 scores for train and test data\\n            #train_model_score = r2_score(ytrain,y_train_pred)\\n            test_model_score = r2_score(y_test,y_test_pred)'),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\utils\\\\utils.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"report[list(models.keys())[i]] =  test_model_score\\n\\n        return report\\n\\n    except Exception as e:\\n        logging.info('Exception occured during model training')\\n        raise CustomException(e,sys)\"),\n",
       " Document(metadata={'source': 'test_dir\\\\src\\\\utils\\\\utils.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"def load_object(file_path):\\n    try:\\n        with open(file_path,'rb') as file_obj:\\n            return pickle.load(file_obj)\\n    except Exception as e:\\n        logging.info('Exception Occured in load_object function utils')\\n        raise CustomException(e,sys)\")]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suraj\\AppData\\Local\\Temp\\ipykernel_3488\\3409896792.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\Users\\Suraj\\Desktop\\Python\\source-code-analysis-gen-ai\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(texts, embedding=embeddings, persist_directory='./db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suraj\\AppData\\Local\\Temp\\ipykernel_3488\\3711397106.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",\n",
    "    api_key=GROQ_API_KEY,\n",
    "    model_name=\"llama-3.1-70b-versatile\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":8}), memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is data_transformation function ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `data_transformation` function is not explicitly defined in the provided code snippet. However, there is a class named `DataTransformation` and a method `get_data_transformation` which seems to be related to data transformation.\n",
      "\n",
      "The `get_data_transformation` method is used to perform data transformation. It creates a `ColumnTransformer` object that applies different transformations to different columns in the data. \n",
      "\n",
      "The `ColumnTransformer` is composed of two pipelines:\n",
      "- `num_pipeline` for numerical columns (`carat`, `depth`, `table`, `x`, `y`, `z`)\n",
      "- `cat_pipeline` for categorical columns (`cut`, `color`, `clarity`)\n",
      "\n",
      "The transformed data is then returned by the `get_data_transformation` method.\n",
      "\n",
      "It's likely that the `data_transformation` function is supposed to call this `get_data_transformation` method or perform a similar task. However, without the actual definition of `data_transformation`, it's impossible to be certain.\n"
     ]
    }
   ],
   "source": [
    "result = qa(question)\n",
    "\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
